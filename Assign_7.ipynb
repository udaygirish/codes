{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assign_7.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"M29a1Gu537eW","colab_type":"code","outputId":"63efd856-7af5-4d5e-905a-a7eda9760687","colab":{"base_uri":"https://localhost:8080/","height":208},"executionInfo":{"status":"ok","timestamp":1562742037433,"user_tz":-330,"elapsed":5284,"user":{"displayName":"Uday Girish Maradana","photoUrl":"https://lh3.googleusercontent.com/-Iol9NOPBRrI/AAAAAAAAAAI/AAAAAAAAAAc/0-nH6NBDU9I/s64/photo.jpg","userId":"07701900522861606838"}}},"source":["!wget https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2019-07-10 07:00:34--  https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n","Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n","Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4573338 (4.4M) [text/plain]\n","Saving to: ‘shakespeare_input.txt.1’\n","\n","shakespeare_input.t 100%[===================>]   4.36M  5.09MB/s    in 0.9s    \n","\n","2019-07-10 07:00:35 (5.09 MB/s) - ‘shakespeare_input.txt.1’ saved [4573338/4573338]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yyFoXvHQ9K55","colab_type":"code","outputId":"0cae32e5-2a62-409c-f271-d0371a1b6671","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1562764144790,"user_tz":-330,"elapsed":5747302,"user":{"displayName":"Uday Girish Maradana","photoUrl":"https://lh3.googleusercontent.com/-Iol9NOPBRrI/AAAAAAAAAAI/AAAAAAAAAAc/0-nH6NBDU9I/s64/photo.jpg","userId":"07701900522861606838"}}},"source":["import numpy\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.layers import RNN\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils\n","filename = \"./shakespeare_input.txt\"\n","raw_text = open(filename).read()\n","raw_text = raw_text.lower()\n","# create mapping of unique chars to integers\n","chars = sorted(list(set(raw_text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))\n","# summarize the loaded data\n","n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print(\"Total Characters: \", n_chars)\n","print(\"Total Vocab: \", n_vocab)\n","seq_length = 200\n","dataX = []\n","dataY = []\n","for i in range(0, n_chars - seq_length, 20):\n","\tseq_in = raw_text[i:i + seq_length]\n","\tseq_out = raw_text[i + seq_length]\n","\tdataX.append([char_to_int[char] for char in seq_in])\n","\tdataY.append(char_to_int[seq_out])\n","n_patterns = len(dataX)\n","print(\"Total Patterns: \", n_patterns)\n","# reshape X to be [samples, time steps, features]\n","X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n","X = X / float(n_vocab)\n","# one hot encode the output variable\n","y = np_utils.to_categorical(dataY)\n","# Model\n","model = Sequential()\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n","model.add(Dropout(0.2))\n","model.add(Dense(y.shape[1], activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","# Checkpoint\n","filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","callbacks_list = [checkpoint]\n","model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Total Characters:  4573338\n","Total Vocab:  41\n","Total Patterns:  228657\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0710 07:00:45.996554 139905521088384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0710 07:00:46.025842 139905521088384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0710 07:00:46.031201 139905521088384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0710 07:00:46.325721 139905521088384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","W0710 07:00:46.334573 139905521088384 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","W0710 07:00:46.358843 139905521088384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0710 07:00:46.383624 139905521088384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","W0710 07:00:46.497744 139905521088384 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/50\n","228657/228657 [==============================] - 456s 2ms/step - loss: 2.9253\n","\n","Epoch 00001: loss improved from inf to 2.92532, saving model to weights-improvement-01-2.9253.hdf5\n","Epoch 2/50\n","228657/228657 [==============================] - 451s 2ms/step - loss: 2.7599\n","\n","Epoch 00002: loss improved from 2.92532 to 2.75992, saving model to weights-improvement-02-2.7599.hdf5\n","Epoch 3/50\n","228657/228657 [==============================] - 451s 2ms/step - loss: 2.7028\n","\n","Epoch 00003: loss improved from 2.75992 to 2.70285, saving model to weights-improvement-03-2.7028.hdf5\n","Epoch 4/50\n","228657/228657 [==============================] - 450s 2ms/step - loss: 2.6526\n","\n","Epoch 00004: loss improved from 2.70285 to 2.65262, saving model to weights-improvement-04-2.6526.hdf5\n","Epoch 5/50\n","228657/228657 [==============================] - 453s 2ms/step - loss: 2.6078\n","\n","Epoch 00005: loss improved from 2.65262 to 2.60780, saving model to weights-improvement-05-2.6078.hdf5\n","Epoch 6/50\n","228657/228657 [==============================] - 453s 2ms/step - loss: 2.5659\n","\n","Epoch 00006: loss improved from 2.60780 to 2.56591, saving model to weights-improvement-06-2.5659.hdf5\n","Epoch 7/50\n","228657/228657 [==============================] - 450s 2ms/step - loss: 2.5267\n","\n","Epoch 00007: loss improved from 2.56591 to 2.52669, saving model to weights-improvement-07-2.5267.hdf5\n","Epoch 8/50\n","228657/228657 [==============================] - 451s 2ms/step - loss: 2.4919\n","\n","Epoch 00008: loss improved from 2.52669 to 2.49186, saving model to weights-improvement-08-2.4919.hdf5\n","Epoch 9/50\n","228657/228657 [==============================] - 452s 2ms/step - loss: 2.4581\n","\n","Epoch 00009: loss improved from 2.49186 to 2.45809, saving model to weights-improvement-09-2.4581.hdf5\n","Epoch 10/50\n","228657/228657 [==============================] - 451s 2ms/step - loss: 2.4281\n","\n","Epoch 00010: loss improved from 2.45809 to 2.42811, saving model to weights-improvement-10-2.4281.hdf5\n","Epoch 11/50\n","228657/228657 [==============================] - 451s 2ms/step - loss: 2.3985\n","\n","Epoch 00011: loss improved from 2.42811 to 2.39852, saving model to weights-improvement-11-2.3985.hdf5\n","Epoch 12/50\n","228657/228657 [==============================] - 455s 2ms/step - loss: 2.3736\n","\n","Epoch 00012: loss improved from 2.39852 to 2.37365, saving model to weights-improvement-12-2.3736.hdf5\n","Epoch 13/50\n","228657/228657 [==============================] - 454s 2ms/step - loss: 2.3482\n","\n","Epoch 00013: loss improved from 2.37365 to 2.34819, saving model to weights-improvement-13-2.3482.hdf5\n","Epoch 14/50\n","228657/228657 [==============================] - 452s 2ms/step - loss: 2.3247\n","\n","Epoch 00014: loss improved from 2.34819 to 2.32467, saving model to weights-improvement-14-2.3247.hdf5\n","Epoch 15/50\n","228657/228657 [==============================] - 451s 2ms/step - loss: 2.3048\n","\n","Epoch 00015: loss improved from 2.32467 to 2.30476, saving model to weights-improvement-15-2.3048.hdf5\n","Epoch 16/50\n","228657/228657 [==============================] - 451s 2ms/step - loss: 2.2859\n","\n","Epoch 00016: loss improved from 2.30476 to 2.28589, saving model to weights-improvement-16-2.2859.hdf5\n","Epoch 17/50\n","228657/228657 [==============================] - 451s 2ms/step - loss: 2.2665\n","\n","Epoch 00017: loss improved from 2.28589 to 2.26651, saving model to weights-improvement-17-2.2665.hdf5\n","Epoch 18/50\n","228657/228657 [==============================] - 451s 2ms/step - loss: 2.2496\n","\n","Epoch 00018: loss improved from 2.26651 to 2.24960, saving model to weights-improvement-18-2.2496.hdf5\n","Epoch 19/50\n","228657/228657 [==============================] - 453s 2ms/step - loss: 2.2329\n","\n","Epoch 00019: loss improved from 2.24960 to 2.23289, saving model to weights-improvement-19-2.2329.hdf5\n","Epoch 20/50\n","228657/228657 [==============================] - 452s 2ms/step - loss: 2.2161\n","\n","Epoch 00020: loss improved from 2.23289 to 2.21607, saving model to weights-improvement-20-2.2161.hdf5\n","Epoch 21/50\n","228657/228657 [==============================] - 452s 2ms/step - loss: 2.2009\n","\n","Epoch 00021: loss improved from 2.21607 to 2.20089, saving model to weights-improvement-21-2.2009.hdf5\n","Epoch 22/50\n","228657/228657 [==============================] - 453s 2ms/step - loss: 2.1852\n","\n","Epoch 00022: loss improved from 2.20089 to 2.18515, saving model to weights-improvement-22-2.1852.hdf5\n","Epoch 23/50\n","228657/228657 [==============================] - 451s 2ms/step - loss: 2.1712\n","\n","Epoch 00023: loss improved from 2.18515 to 2.17122, saving model to weights-improvement-23-2.1712.hdf5\n","Epoch 24/50\n","228657/228657 [==============================] - 451s 2ms/step - loss: 2.1585\n","\n","Epoch 00024: loss improved from 2.17122 to 2.15849, saving model to weights-improvement-24-2.1585.hdf5\n","Epoch 25/50\n","228657/228657 [==============================] - 452s 2ms/step - loss: 2.1457\n","\n","Epoch 00025: loss improved from 2.15849 to 2.14567, saving model to weights-improvement-25-2.1457.hdf5\n","Epoch 26/50\n","228657/228657 [==============================] - 452s 2ms/step - loss: 2.1359\n","\n","Epoch 00026: loss improved from 2.14567 to 2.13593, saving model to weights-improvement-26-2.1359.hdf5\n","Epoch 27/50\n","228657/228657 [==============================] - 452s 2ms/step - loss: 2.1245\n","\n","Epoch 00027: loss improved from 2.13593 to 2.12451, saving model to weights-improvement-27-2.1245.hdf5\n","Epoch 28/50\n","228657/228657 [==============================] - 452s 2ms/step - loss: 2.1105\n","\n","Epoch 00028: loss improved from 2.12451 to 2.11045, saving model to weights-improvement-28-2.1105.hdf5\n","Epoch 29/50\n","228657/228657 [==============================] - 451s 2ms/step - loss: 2.1006\n","\n","Epoch 00029: loss improved from 2.11045 to 2.10063, saving model to weights-improvement-29-2.1006.hdf5\n","Epoch 30/50\n","228657/228657 [==============================] - 451s 2ms/step - loss: 2.0912\n","\n","Epoch 00030: loss improved from 2.10063 to 2.09118, saving model to weights-improvement-30-2.0912.hdf5\n","Epoch 31/50\n","228657/228657 [==============================] - 450s 2ms/step - loss: 2.0815\n","\n","Epoch 00031: loss improved from 2.09118 to 2.08145, saving model to weights-improvement-31-2.0815.hdf5\n","Epoch 32/50\n","228657/228657 [==============================] - 453s 2ms/step - loss: 2.0723\n","\n","Epoch 00032: loss improved from 2.08145 to 2.07234, saving model to weights-improvement-32-2.0723.hdf5\n","Epoch 33/50\n","228657/228657 [==============================] - 453s 2ms/step - loss: 2.0636\n","\n","Epoch 00033: loss improved from 2.07234 to 2.06363, saving model to weights-improvement-33-2.0636.hdf5\n","Epoch 34/50\n","228657/228657 [==============================] - 454s 2ms/step - loss: 2.0566\n","\n","Epoch 00034: loss improved from 2.06363 to 2.05664, saving model to weights-improvement-34-2.0566.hdf5\n","Epoch 35/50\n"," 46336/228657 [=====>........................] - ETA: 6:00 - loss: 2.0206Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kEKbfdb-pp_Z","colab_type":"code","colab":{}},"source":["#!gsutil -m cp -r  gs://proobj_1/ ./w* (for getting previously trained weights)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gzT9n4-Pvg2V","colab_type":"code","colab":{}},"source":["filename = \"weights-improvement-05-2.7328.hdf5\"\n","model.load_weights(filename)\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iyXzHm6Qxdxu","colab_type":"code","colab":{}},"source":["int_to_char = dict((i, c) for i, c in enumerate(chars))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zsDUNFJ3xhtc","colab_type":"code","colab":{}},"source":["# For generating character level text\n","import os\n","import sys\n","start = numpy.random.randint(0, len(dataX)-1)\n","pattern = dataX[start]\n","print(\"Seed:\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n","for i in range(1000):\n","\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n","\tx = x / float(n_vocab)\n","\tprediction = model.predict(x, verbose=0)\n","\tindex = numpy.argmax(prediction)\n","\tresult = int_to_char[index]\n","\tseq_in = [int_to_char[value] for value in pattern]\n","\tsys.stdout.write(result)\n","\tpattern.append(index)\n","\tpattern = pattern[1:len(pattern)]\n","print(\"\\nDone.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SXH257Pqxv1N","colab_type":"code","colab":{}},"source":["!gsutil -m cp -r w* gs://proobj_1/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"knudRvFf0BMD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}